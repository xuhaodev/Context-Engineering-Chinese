{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e71c7e",
   "metadata": {},
   "source": [
    "Context-Engineering: RAG 检索增强生成的实践配方\n",
    "=============================================\n",
    "\n",
    "本模块演示了检索增强生成（RAG）模式的实际实现，用于通过外部知识增强大语言模型（LLM）的上下文。\n",
    "我们专注于最小化、高效的实现，突出关键概念，无需复杂的基础设施。\n",
    "\n",
    "涵盖的关键概念：\n",
    "1. 基本 RAG 流水线构建\n",
    "2. 上下文窗口管理与分块策略\n",
    "3. 嵌入与检索技术\n",
    "4. 检索质量与相关性评估\n",
    "5. 上下文集成模式\n",
    "6. 高级 RAG 变体\n",
    "\n",
    "用法：\n",
    "    # 在 Jupyter 或 Colab 中：\n",
    "    %run 04_rag_recipes.py\n",
    "    # 或\n",
    "    from rag_recipes import SimpleRAG, ChunkedRAG, HybridRAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02217ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 09:54:06,086 - faiss.loader - INFO - Loading faiss with AVX512 support.\n",
      "2025-07-12 09:54:06,086 - faiss.loader - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2025-07-12 09:54:06,086 - faiss.loader - INFO - Loading faiss with AVX2 support.\n",
      "2025-07-12 09:54:06,086 - faiss.loader - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2025-07-12 09:54:06,086 - faiss.loader - INFO - Loading faiss with AVX2 support.\n",
      "2025-07-12 09:54:06,118 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-07-12 09:54:06,129 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "import tiktoken\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union, Callable, TypeVar\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 检查必需的库\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    logger.warning(\"未找到 OpenAI 包。请安装：pip install openai\")\n",
    "\n",
    "try:\n",
    "    import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    ENV_LOADED = True\n",
    "except ImportError:\n",
    "    ENV_LOADED = False\n",
    "    logger.warning(\"未找到 python-dotenv。请安装：pip install python-dotenv\")\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    logger.warning(\"未找到 scikit-learn。请安装：pip install scikit-learn\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    NUMPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NUMPY_AVAILABLE = False\n",
    "    logger.warning(\"未找到 NumPy。请安装：pip install numpy\")\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "    logger.warning(\"未找到 FAISS。请安装：pip install faiss-cpu 或 faiss-gpu\")\n",
    "\n",
    "# 常量\n",
    "DEFAULT_MODEL = \"openai/gpt-4.1\"\n",
    "DEFAULT_EMBEDDING_MODEL = \"openai/text-embedding-3-small\"\n",
    "DEFAULT_TEMPERATURE = 0.7\n",
    "DEFAULT_MAX_TOKENS = 500\n",
    "DEFAULT_CHUNK_SIZE = 1000\n",
    "DEFAULT_CHUNK_OVERLAP = 200\n",
    "DEFAULT_TOP_K = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2455961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础数据结构\n",
    "# =====================\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"表示一个文档或文本块及其元数据。\"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = None\n",
    "    embedding: Optional[List[float]] = None\n",
    "    id: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"如果未提供，初始化默认值。\"\"\"\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "        \n",
    "        if self.id is None:\n",
    "            # 基于内容哈希生成简单ID\n",
    "            import hashlib\n",
    "            self.id = hashlib.md5(self.content.encode()).hexdigest()[:8]\n",
    "\n",
    "\n",
    "# 辅助函数\n",
    "# ===============\n",
    "\n",
    "def setup_client(api_key=None, model=DEFAULT_MODEL):\n",
    "    \"\"\"\n",
    "    设置用于 LLM 交互的 API 客户端。\n",
    "\n",
    "    参数：\n",
    "        api_key: API 密钥（如果为 None，将在环境变量中查找 OPENAI_API_KEY）\n",
    "        model: 要使用的模型名称\n",
    "\n",
    "    返回：\n",
    "        tuple: (客户端, 模型名称)\n",
    "    \"\"\"\n",
    "    if api_key is None:\n",
    "        api_key = os.environ.get(\"GITHUB_TOKEN\")\n",
    "        if api_key is None and not ENV_LOADED:\n",
    "            logger.warning(\"未找到 API 密钥。请设置 OPENAI_API_KEY 环境变量或传递 api_key 参数。\")\n",
    "    \n",
    "    if OPENAI_AVAILABLE:\n",
    "        client = OpenAI(\n",
    "            base_url=\"https://models.github.ai/inference\",\n",
    "            api_key=api_key,\n",
    "        )\n",
    "        return client, model\n",
    "    else:\n",
    "        logger.error(\"需要 OpenAI 包。请安装：pip install openai\")\n",
    "        return None, model\n",
    "\n",
    "\n",
    "def count_tokens(text: str, model: str = DEFAULT_MODEL) -> int:\n",
    "    \"\"\"\n",
    "    使用合适的分词器计算文本字符串中的令牌数。\n",
    "\n",
    "    参数：\n",
    "        text: 要分词的文本\n",
    "        model: 用于分词的模型名称\n",
    "\n",
    "    返回：\n",
    "        int: 令牌数量\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        return len(encoding.encode(text))\n",
    "    except Exception as e:\n",
    "        # 当 tiktoken 不支持该模型时的备用方案\n",
    "        logger.warning(f\"无法为 {model} 使用 tiktoken：{e}\")\n",
    "        # 粗略近似：英语中 1 个令牌 ≈ 4 个字符\n",
    "        return len(text) // 4\n",
    "\n",
    "\n",
    "def generate_embedding(\n",
    "    text: str,\n",
    "    client=None,\n",
    "    model: str = DEFAULT_EMBEDDING_MODEL\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    为给定文本生成嵌入向量。\n",
    "\n",
    "    参数：\n",
    "        text: 要嵌入的文本\n",
    "        client: API 客户端（如果为 None，将创建一个）\n",
    "        model: 嵌入模型名称\n",
    "\n",
    "    返回：\n",
    "        list: 嵌入向量\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        client, _ = setup_client()\n",
    "        if client is None:\n",
    "            # 如果没有可用的客户端，返回虚拟嵌入\n",
    "            return [0.0] * 1536  # 许多嵌入模型的默认大小\n",
    "    \n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=[text]\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logger.error(f\"生成嵌入时出错：{e}\")\n",
    "        # 出错时返回虚拟嵌入\n",
    "        return [0.0] * 1536\n",
    "\n",
    "\n",
    "def generate_response(\n",
    "    prompt: str,\n",
    "    client=None,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = DEFAULT_TEMPERATURE,\n",
    "    max_tokens: int = DEFAULT_MAX_TOKENS,\n",
    "    system_message: str = \"你是一个有用的助手。\"\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    从 LLM 生成响应并返回元数据。\n",
    "\n",
    "    参数：\n",
    "        prompt: 要发送的提示\n",
    "        client: API 客户端（如果为 None，将创建一个）\n",
    "        model: 模型名称\n",
    "        temperature: 温度参数\n",
    "        max_tokens: 生成的最大令牌数\n",
    "        system_message: 要使用的系统消息\n",
    "\n",
    "    返回：\n",
    "        tuple: (响应文本, 元数据)\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        client, model = setup_client(model=model)\n",
    "        if client is None:\n",
    "            return \"错误：没有可用的 API 客户端\", {\"error\": \"没有 API 客户端\"}\n",
    "    \n",
    "    prompt_tokens = count_tokens(prompt, model)\n",
    "    system_tokens = count_tokens(system_message, model)\n",
    "    \n",
    "    metadata = {\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"system_tokens\": system_tokens,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        response_tokens = count_tokens(response_text, model)\n",
    "        \n",
    "        metadata.update({\n",
    "            \"latency\": latency,\n",
    "            \"response_tokens\": response_tokens,\n",
    "            \"total_tokens\": prompt_tokens + system_tokens + response_tokens,\n",
    "            \"token_efficiency\": response_tokens / (prompt_tokens + system_tokens) if (prompt_tokens + system_tokens) > 0 else 0,\n",
    "            \"tokens_per_second\": response_tokens / latency if latency > 0 else 0\n",
    "        })\n",
    "        \n",
    "        return response_text, metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"生成响应时出错：{e}\")\n",
    "        metadata[\"error\"] = str(e)\n",
    "        return f\"错误：{str(e)}\", metadata\n",
    "\n",
    "\n",
    "def format_metrics(metrics: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    将指标字典格式化为可读字符串。\n",
    "    \n",
    "    参数：\n",
    "        metrics: 指标字典\n",
    "        \n",
    "    返回：\n",
    "        str: 格式化的指标字符串\n",
    "    \"\"\"\n",
    "    # 选择要显示的最重要指标\n",
    "    key_metrics = {\n",
    "        \"提示令牌\": metrics.get(\"prompt_tokens\", 0),\n",
    "        \"响应令牌\": metrics.get(\"response_tokens\", 0),\n",
    "        \"总令牌\": metrics.get(\"total_tokens\", 0),\n",
    "        \"延迟\": f\"{metrics.get('latency', 0):.2f}秒\",\n",
    "        \"令牌效率\": f\"{metrics.get('token_efficiency', 0):.2f}\"\n",
    "    }\n",
    "    \n",
    "    return \" | \".join([f\"{k}: {v}\" for k, v in key_metrics.items()])\n",
    "\n",
    "\n",
    "def display_response(\n",
    "    prompt: str,\n",
    "    response: str,\n",
    "    retrieved_context: Optional[str] = None,\n",
    "    metrics: Dict[str, Any] = None,\n",
    "    show_prompt: bool = True,\n",
    "    show_context: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    在笔记本中显示提示-响应对和指标。\n",
    "    \n",
    "    参数：\n",
    "        prompt: 提示文本\n",
    "        response: 响应文本\n",
    "        retrieved_context: 检索到的上下文（可选）\n",
    "        metrics: 指标字典（可选）\n",
    "        show_prompt: 是否显示提示文本\n",
    "        show_context: 是否显示检索到的上下文\n",
    "    \"\"\"\n",
    "    if show_prompt:\n",
    "        display(HTML(\"<h4>查询：</h4>\"))\n",
    "        display(Markdown(f\"```\\n{prompt}\\n```\"))\n",
    "    \n",
    "    if retrieved_context and show_context:\n",
    "        display(HTML(\"<h4>检索到的上下文：</h4>\"))\n",
    "        display(Markdown(f\"```\\n{retrieved_context}\\n```\"))\n",
    "    \n",
    "    display(HTML(\"<h4>响应：</h4>\"))\n",
    "    display(Markdown(response))\n",
    "    \n",
    "    if metrics:\n",
    "        display(HTML(\"<h4>指标：</h4>\"))\n",
    "        display(Markdown(f\"```\\n{format_metrics(metrics)}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档处理函数\n",
    "# ============================\n",
    "\n",
    "def text_to_chunks(\n",
    "    text: str,\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
    "    model: str = DEFAULT_MODEL\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    将文本分割成指定标记大小的重叠块。\n",
    "    \n",
    "    Args:\n",
    "        text: 要分割的文本\n",
    "        chunk_size: 每个块的最大标记数\n",
    "        chunk_overlap: 块之间重叠的标记数\n",
    "        model: 用于标记化的模型\n",
    "        \n",
    "    Returns:\n",
    "        list: Document对象列表\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # 获取标记器\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except:\n",
    "        logger.warning(f\"无法获取{model}的标记器。使用近似分块。\")\n",
    "        return _approximate_text_to_chunks(text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # 对文本进行标记化\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    # 创建块\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # 提取块标记\n",
    "        chunk_end = min(i + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[i:chunk_end]\n",
    "        \n",
    "        # 解码回文本\n",
    "        chunk_text = encoding.decode(chunk_tokens)\n",
    "        \n",
    "        # 创建文档\n",
    "        chunks.append(Document(\n",
    "            content=chunk_text,\n",
    "            metadata={\n",
    "                \"start_idx\": i,\n",
    "                \"end_idx\": chunk_end,\n",
    "                \"chunk_size\": len(chunk_tokens)\n",
    "            }\n",
    "        ))\n",
    "        \n",
    "        # 移动到下一个块，考虑重叠\n",
    "        i += max(1, chunk_size - chunk_overlap)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _approximate_text_to_chunks(\n",
    "    text: str,\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    使用简单的基于字符的近似方法将文本分割成块。\n",
    "    \n",
    "    Args:\n",
    "        text: 要分割的文本\n",
    "        chunk_size: 每个块的近似字符数（假设约4个字符/标记）\n",
    "        chunk_overlap: 重叠的近似字符数\n",
    "        \n",
    "    Returns:\n",
    "        list: Document对象列表\n",
    "    \"\"\"\n",
    "    # 将标记大小转换为字符大小（近似）\n",
    "    char_size = chunk_size * 4\n",
    "    char_overlap = chunk_overlap * 4\n",
    "    \n",
    "    # 首先按段落分割（如果可能的话，避免在段落中间分割）\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_size = len(paragraph)\n",
    "        \n",
    "        # 如果添加此段落会超过块大小\n",
    "        if current_size + paragraph_size > char_size and current_chunk:\n",
    "            # 从当前文本创建一个块\n",
    "            chunk_text = '\\n\\n'.join(current_chunk)\n",
    "            chunks.append(Document(\n",
    "                content=chunk_text,\n",
    "                metadata={\"approx_size\": current_size}\n",
    "            ))\n",
    "            \n",
    "            # 开始一个新块，带有重叠\n",
    "            # 找到应该包含在重叠中的段落\n",
    "            overlap_size = 0\n",
    "            overlap_paragraphs = []\n",
    "            \n",
    "            for p in reversed(current_chunk):\n",
    "                p_size = len(p)\n",
    "                if overlap_size + p_size <= char_overlap:\n",
    "                    overlap_paragraphs.insert(0, p)\n",
    "                    overlap_size += p_size\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            current_chunk = overlap_paragraphs\n",
    "            current_size = overlap_size\n",
    "        \n",
    "        # 添加当前段落\n",
    "        current_chunk.append(paragraph)\n",
    "        current_size += paragraph_size\n",
    "    \n",
    "    # 如果还有剩余内容，添加最后一个块\n",
    "    if current_chunk:\n",
    "        chunk_text = '\\n\\n'.join(current_chunk)\n",
    "        chunks.append(Document(\n",
    "            content=chunk_text,\n",
    "            metadata={\"approx_size\": current_size}\n",
    "        ))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_document_batch_embeddings(\n",
    "    documents: List[Document],\n",
    "    client=None,\n",
    "    model: str = DEFAULT_EMBEDDING_MODEL,\n",
    "    batch_size: int = 10\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    高效地为一批文档生成向量嵌入。\n",
    "    \n",
    "    Args:\n",
    "        documents: 要嵌入的文档对象列表\n",
    "        client: API客户端（如果为None，将创建一个）\n",
    "        model: 使用的嵌入模型\n",
    "        batch_size: 每次API调用中嵌入的文档数量\n",
    "        \n",
    "    Returns:\n",
    "        list: 带有嵌入的更新文档对象\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return []\n",
    "    \n",
    "    if client is None:\n",
    "        client, _ = setup_client()\n",
    "        if client is None:\n",
    "            logger.error(\"没有可用的API客户端进行嵌入\")\n",
    "            return documents\n",
    "    \n",
    "    # 分批处理\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_texts = [doc.content for doc in batch]\n",
    "        \n",
    "        try:\n",
    "            # 为批次生成嵌入\n",
    "            response = client.embeddings.create(\n",
    "                model=model,\n",
    "                input=batch_texts\n",
    "            )\n",
    "            \n",
    "            # 使用嵌入更新文档\n",
    "            for j, doc in enumerate(batch):\n",
    "                if j < len(response.data):\n",
    "                    doc.embedding = response.data[j].embedding\n",
    "                else:\n",
    "                    logger.warning(f\"文档 {i+j} 缺少嵌入\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成批次嵌入时出错: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def similarity_search(\n",
    "    query_embedding: List[float],\n",
    "    documents: List[Document],\n",
    "    top_k: int = DEFAULT_TOP_K\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    找到与查询嵌入最相似的文档。\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: 查询嵌入向量\n",
    "        documents: 带有嵌入的文档对象列表\n",
    "        top_k: 返回的结果数量\n",
    "        \n",
    "    Returns:\n",
    "        list: (文档, 相似度分数) 元组列表\n",
    "    \"\"\"\n",
    "    if not NUMPY_AVAILABLE:\n",
    "        logger.error(\"相似度搜索需要NumPy\")\n",
    "        return []\n",
    "    \n",
    "    # 过滤掉没有嵌入的文档\n",
    "    docs_with_embeddings = [doc for doc in documents if doc.embedding is not None]\n",
    "    \n",
    "    if not docs_with_embeddings:\n",
    "        logger.warning(\"找不到带有嵌入的文档\")\n",
    "        return []\n",
    "    \n",
    "    # 将嵌入转换为numpy数组\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n",
    "    doc_embeddings = np.array([doc.embedding for doc in docs_with_embeddings])\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    if SKLEARN_AVAILABLE:\n",
    "        similarities = cosine_similarity(query_embedding_np, doc_embeddings)[0]\n",
    "    else:\n",
    "        # 回退到手动余弦相似度计算\n",
    "        norm_query = np.linalg.norm(query_embedding_np)\n",
    "        norm_docs = np.linalg.norm(doc_embeddings, axis=1)\n",
    "        dot_products = np.dot(query_embedding_np, doc_embeddings.T)[0]\n",
    "        similarities = dot_products / (norm_query * norm_docs)\n",
    "    \n",
    "    # 创建(文档, 相似度)对\n",
    "    doc_sim_pairs = list(zip(docs_with_embeddings, similarities))\n",
    "    \n",
    "    # 按相似度排序（降序）并取前top_k个\n",
    "    sorted_pairs = sorted(doc_sim_pairs, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_pairs[:top_k]\n",
    "\n",
    "\n",
    "def create_faiss_index(documents: List[Document]) -> Any:\n",
    "    \"\"\"\n",
    "    从文档嵌入创建FAISS索引以进行高效的相似度搜索。\n",
    "    \n",
    "    Args:\n",
    "        documents: 带有嵌入的文档对象列表\n",
    "        \n",
    "    Returns:\n",
    "        object: FAISS索引，如果FAISS不可用则返回None\n",
    "    \"\"\"\n",
    "    if not FAISS_AVAILABLE:\n",
    "        logger.error(\"索引需要FAISS\")\n",
    "        return None\n",
    "    \n",
    "    # 过滤掉没有嵌入的文档\n",
    "    docs_with_embeddings = [doc for doc in documents if doc.embedding is not None]\n",
    "    \n",
    "    if not docs_with_embeddings:\n",
    "        logger.warning(\"找不到带有嵌入的文档\")\n",
    "        return None\n",
    "    \n",
    "    # 从第一个文档获取嵌入维度\n",
    "    embedding_dim = len(docs_with_embeddings[0].embedding)\n",
    "    \n",
    "    # 创建FAISS索引\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "    \n",
    "    # 将嵌入添加到索引中\n",
    "    embeddings = np.array([doc.embedding for doc in docs_with_embeddings], dtype=np.float32)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index, docs_with_embeddings\n",
    "\n",
    "\n",
    "def faiss_similarity_search(\n",
    "    query_embedding: List[float],\n",
    "    faiss_index: Any,\n",
    "    documents: List[Document],\n",
    "    top_k: int = DEFAULT_TOP_K\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    使用FAISS索引找到最相似的文档。\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: 查询嵌入向量\n",
    "        faiss_index: FAISS索引（来自create_faiss_index）\n",
    "        documents: 对应索引的文档对象列表\n",
    "        top_k: 返回的结果数量\n",
    "        \n",
    "    Returns:\n",
    "        list: (文档, 相似度分数) 元组列表\n",
    "    \"\"\"\n",
    "    if not FAISS_AVAILABLE:\n",
    "        logger.error(\"相似度搜索需要FAISS\")\n",
    "        return []\n",
    "    \n",
    "    if faiss_index is None:\n",
    "        logger.error(\"FAISS索引为None\")\n",
    "        return []\n",
    "    \n",
    "    # 如果从create_faiss_index返回，解包索引和文档\n",
    "    if isinstance(faiss_index, tuple):\n",
    "        index, docs_with_embeddings = faiss_index\n",
    "    else:\n",
    "        index = faiss_index\n",
    "        docs_with_embeddings = documents\n",
    "    \n",
    "    # 将查询转换为numpy数组\n",
    "    query_np = np.array([query_embedding], dtype=np.float32)\n",
    "    \n",
    "    # 搜索索引\n",
    "    distances, indices = index.search(query_np, top_k)\n",
    "    \n",
    "    # 创建(文档, 相似度)对\n",
    "    # 将L2距离转换为相似度分数（越高越好）\n",
    "    results = []\n",
    "    for i in range(len(indices[0])):\n",
    "        idx = indices[0][i]\n",
    "        if idx < len(docs_with_embeddings):\n",
    "            # 将L2距离转换为相似度（1 / (1 + distance)）\n",
    "            similarity = 1.0 / (1.0 + distances[0][i])\n",
    "            results.append((docs_with_embeddings[idx], similarity))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 系统基类\n",
    "# ============\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    检索增强生成系统的基类。\n",
    "    提供通用功能和接口。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client=None,\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n",
    "        system_message: str = \"你是一个基于检索上下文来回答问题的有用助手。\",\n",
    "        max_tokens: int = DEFAULT_MAX_TOKENS,\n",
    "        temperature: float = DEFAULT_TEMPERATURE,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化 RAG 系统。\n",
    "        \n",
    "        参数:\n",
    "            client: API 客户端（如果为 None，将创建一个新的）\n",
    "            model: 用于生成的模型名称\n",
    "            embedding_model: 用于嵌入的模型名称\n",
    "            system_message: 要使用的系统消息\n",
    "            max_tokens: 最大生成令牌数\n",
    "            temperature: 温度参数\n",
    "            verbose: 是否打印调试信息\n",
    "        \"\"\"\n",
    "        self.client, self.model = setup_client(model=model) if client is None else (client, model)\n",
    "        self.embedding_model = embedding_model\n",
    "        self.system_message = system_message\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # 初始化文档存储\n",
    "        self.documents = []\n",
    "        \n",
    "        # 初始化历史记录和指标跟踪\n",
    "        self.history = []\n",
    "        self.metrics = {\n",
    "            \"total_prompt_tokens\": 0,\n",
    "            \"total_response_tokens\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"total_latency\": 0,\n",
    "            \"retrieval_latency\": 0,\n",
    "            \"queries\": 0\n",
    "        }\n",
    "    \n",
    "    def _log(self, message: str) -> None:\n",
    "        \"\"\"\n",
    "        如果启用了详细模式，则记录消息。\n",
    "        \n",
    "        参数:\n",
    "            message: 要记录的消息\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            logger.info(message)\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        将文档添加到文档存储中。\n",
    "        \n",
    "        参数:\n",
    "            documents: 要添加的文档对象列表\n",
    "        \"\"\"\n",
    "        self.documents.extend(documents)\n",
    "    \n",
    "    def add_texts(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        metadatas: Optional[List[Dict[str, Any]]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        将文本添加到文档存储中，可选择添加元数据。\n",
    "        \n",
    "        参数:\n",
    "            texts: 要添加的文本字符串列表\n",
    "            metadatas: 元数据字典列表（可选）\n",
    "        \"\"\"\n",
    "        if metadatas is None:\n",
    "            metadatas = [{} for _ in texts]\n",
    "        \n",
    "        # 创建 Document 对象\n",
    "        documents = [\n",
    "            Document(content=text, metadata=metadata)\n",
    "            for text, metadata in zip(texts, metadatas)\n",
    "        ]\n",
    "        \n",
    "        self.add_documents(documents)\n",
    "    \n",
    "    def _retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = DEFAULT_TOP_K\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        检索与查询相关的文档。\n",
    "        \n",
    "        参数:\n",
    "            query: 查询字符串\n",
    "            top_k: 返回的结果数量\n",
    "            \n",
    "        返回:\n",
    "            list: (文档, 相似度分数) 元组列表\n",
    "        \"\"\"\n",
    "        # 这是一个占位符 - 子类应该实现这个方法\n",
    "        raise NotImplementedError(\"子类必须实现 _retrieve 方法\")\n",
    "    \n",
    "    def _format_context(\n",
    "        self,\n",
    "        retrieved_documents: List[Tuple[Document, float]]\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        将检索到的文档格式化为上下文字符串。\n",
    "        \n",
    "        参数:\n",
    "            retrieved_documents: (文档, 相似度分数) 元组列表\n",
    "            \n",
    "        返回:\n",
    "            str: 格式化的上下文字符串\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, (doc, score) in enumerate(retrieved_documents):\n",
    "            # 格式化文档和元数据\n",
    "            source_info = \"\"\n",
    "            if doc.metadata:\n",
    "                # 如果可用，提取来源信息\n",
    "                source = doc.metadata.get(\"source\", \"\")\n",
    "                if source:\n",
    "                    source_info = f\" (来源: {source})\"\n",
    "            \n",
    "            context_parts.append(f\"[文档 {i+1}{source_info}]\\n{doc.content}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _create_prompt(\n",
    "        self,\n",
    "        query: str,\n",
    "        context: str\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        创建结合查询和检索上下文的提示。\n",
    "        \n",
    "        参数:\n",
    "            query: 用户查询\n",
    "            context: 检索到的上下文\n",
    "            \n",
    "        返回:\n",
    "            str: 格式化的提示\n",
    "        \"\"\"\n",
    "        return f\"\"\"基于检索到的上下文回答以下问题。如果上下文不包含相关信息，请如实说明而不是编造答案。\n",
    "\n",
    "检索到的上下文:\n",
    "{context}\n",
    "\n",
    "问题: {query}\n",
    "\n",
    "答案:\"\"\"\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = DEFAULT_TOP_K\n",
    "    ) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        通过 RAG 流水线处理查询。\n",
    "        \n",
    "        参数:\n",
    "            query: 查询字符串\n",
    "            top_k: 返回的结果数量\n",
    "            \n",
    "        返回:\n",
    "            tuple: (响应, 详细信息)\n",
    "        \"\"\"\n",
    "        self._log(f\"正在处理查询: {query}\")\n",
    "        \n",
    "        # 检索相关文档\n",
    "        start_time = time.time()\n",
    "        retrieved_docs = self._retrieve(query, top_k)\n",
    "        retrieval_latency = time.time() - start_time\n",
    "        \n",
    "        # 从检索到的文档格式化上下文\n",
    "        context = self._format_context(retrieved_docs)\n",
    "        \n",
    "        # 创建提示\n",
    "        prompt = self._create_prompt(query, context)\n",
    "        \n",
    "        # 生成响应\n",
    "        response, metadata = generate_response(\n",
    "            prompt=prompt,\n",
    "            client=self.client,\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            system_message=self.system_message\n",
    "        )\n",
    "        \n",
    "        # 更新指标\n",
    "        self.metrics[\"total_prompt_tokens\"] += metadata.get(\"prompt_tokens\", 0)\n",
    "        self.metrics[\"total_response_tokens\"] += metadata.get(\"response_tokens\", 0)\n",
    "        self.metrics[\"total_tokens\"] += metadata.get(\"total_tokens\", 0)\n",
    "        self.metrics[\"total_latency\"] += metadata.get(\"latency\", 0)\n",
    "        self.metrics[\"retrieval_latency\"] += retrieval_latency\n",
    "        self.metrics[\"queries\"] += 1\n",
    "        \n",
    "        # 添加到历史记录\n",
    "        query_record = {\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": [(doc.content, score) for doc, score in retrieved_docs],\n",
    "            \"context\": context,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"metrics\": {\n",
    "                **metadata,\n",
    "                \"retrieval_latency\": retrieval_latency\n",
    "            },\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        self.history.append(query_record)\n",
    "        \n",
    "        # 创建详细信息字典\n",
    "        details = {\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "            \"context\": context,\n",
    "            \"response\": response,\n",
    "            \"metrics\": {\n",
    "                **metadata,\n",
    "                \"retrieval_latency\": retrieval_latency\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return response, details\n",
    "    \n",
    "    def get_summary_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        获取所有查询的摘要指标。\n",
    "        \n",
    "        返回:\n",
    "            dict: 摘要指标\n",
    "        \"\"\"\n",
    "        summary = self.metrics.copy()\n",
    "        \n",
    "        # 添加派生指标\n",
    "        if summary[\"queries\"] > 0:\n",
    "            summary[\"avg_latency_per_query\"] = summary[\"total_latency\"] / summary[\"queries\"]\n",
    "            summary[\"avg_retrieval_latency\"] = summary[\"retrieval_latency\"] / summary[\"queries\"]\n",
    "            \n",
    "        if summary[\"total_prompt_tokens\"] > 0:\n",
    "            summary[\"overall_efficiency\"] = (\n",
    "                summary[\"total_response_tokens\"] / summary[\"total_prompt_tokens\"]\n",
    "            )\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def display_query_results(self, details: Dict[str, Any], show_context: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        在笔记本中显示查询结果。\n",
    "        \n",
    "        参数:\n",
    "            details: 来自 query() 的查询详细信息\n",
    "            show_context: 是否显示检索到的上下文\n",
    "        \"\"\"\n",
    "        display(HTML(\"<h2>RAG 查询结果</h2>\"))\n",
    "        \n",
    "        # 显示查询\n",
    "        display(HTML(\"<h3>查询</h3>\"))\n",
    "        display(Markdown(details[\"query\"]))\n",
    "        \n",
    "        # 显示检索到的文档\n",
    "        if show_context and \"retrieved_docs\" in details:\n",
    "            display(HTML(\"<h3>检索到的文档</h3>\"))\n",
    "            \n",
    "            for i, (doc, score) in enumerate(details[\"retrieved_docs\"]):\n",
    "                display(HTML(f\"<h4>文档 {i+1} (分数: {score:.4f})</h4>\"))\n",
    "                \n",
    "                # 如果可用，显示元数据\n",
    "                if doc.metadata:\n",
    "                    display(HTML(\"<p><em>元数据:</em></p>\"))\n",
    "                    display(Markdown(f\"```json\\n{json.dumps(doc.metadata, indent=2)}\\n```\"))\n",
    "                \n",
    "                # 显示内容\n",
    "                display(Markdown(f\"```\\n{doc.content}\\n```\"))\n",
    "        \n",
    "        # 显示响应\n",
    "        display(HTML(\"<h3>响应</h3>\"))\n",
    "        display(Markdown(details[\"response\"]))\n",
    "        \n",
    "        # 显示指标\n",
    "        if \"metrics\" in details:\n",
    "            display(HTML(\"<h3>指标</h3>\"))\n",
    "            metrics = details[\"metrics\"]\n",
    "            \n",
    "            # 格式化指标\n",
    "            display(Markdown(f\"\"\"\n",
    "            - 提示令牌数: {metrics.get('prompt_tokens', 0)}\n",
    "            - 响应令牌数: {metrics.get('response_tokens', 0)}\n",
    "            - 总令牌数: {metrics.get('total_tokens', 0)}\n",
    "            - 生成延迟: {metrics.get('latency', 0):.2f}秒\n",
    "            - 检索延迟: {metrics.get('retrieval_latency', 0):.2f}秒\n",
    "            - 总延迟: {metrics.get('latency', 0) + metrics.get('retrieval_latency', 0):.2f}秒\n",
    "            \"\"\"))\n",
    "    \n",
    "    def visualize_metrics(self) -> None:\n",
    "        \"\"\"\n",
    "        创建跨查询的指标可视化。\n",
    "        \"\"\"\n",
    "        if not self.history:\n",
    "            logger.warning(\"没有历史记录可视化\")\n",
    "            return\n",
    "        \n",
    "        # 提取绘图数据\n",
    "        queries = list(range(1, len(self.history) + 1))\n",
    "        prompt_tokens = [h[\"metrics\"].get(\"prompt_tokens\", 0) for h in self.history]\n",
    "        response_tokens = [h[\"metrics\"].get(\"response_tokens\", 0) for h in self.history]\n",
    "        generation_latencies = [h[\"metrics\"].get(\"latency\", 0) for h in self.history]\n",
    "        retrieval_latencies = [h[\"metrics\"].get(\"retrieval_latency\", 0) for h in self.history]\n",
    "        total_latencies = [g + r for g, r in zip(generation_latencies, retrieval_latencies)]\n",
    "        \n",
    "        # 创建图表\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(\"RAG 系统各查询指标\", fontsize=16)\n",
    "\n",
    "        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "        # 图表1: 令牌使用情况\n",
    "        axes[0, 0].bar(queries, prompt_tokens, label=\"提示令牌\", color=\"blue\", alpha=0.7)\n",
    "        axes[0, 0].bar(queries, response_tokens, bottom=prompt_tokens, \n",
    "                       label=\"响应令牌\", color=\"green\", alpha=0.7)\n",
    "        axes[0, 0].set_title(\"令牌使用情况\")\n",
    "        axes[0, 0].set_xlabel(\"查询\")\n",
    "        axes[0, 0].set_ylabel(\"令牌数\")\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # 图表2: 延迟分解\n",
    "        axes[0, 1].bar(queries, retrieval_latencies, label=\"检索\", color=\"orange\", alpha=0.7)\n",
    "        axes[0, 1].bar(queries, generation_latencies, bottom=retrieval_latencies, \n",
    "                      label=\"生成\", color=\"red\", alpha=0.7)\n",
    "        axes[0, 1].set_title(\"延迟分解\")\n",
    "        axes[0, 1].set_xlabel(\"查询\")\n",
    "        axes[0, 1].set_ylabel(\"秒\")\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # 图表3: 检索数量\n",
    "        if any(\"retrieved_docs\" in h for h in self.history):\n",
    "            doc_counts = [len(h.get(\"retrieved_docs\", [])) for h in self.history]\n",
    "            axes[1, 0].plot(queries, doc_counts, marker='o', color=\"purple\", alpha=0.7)\n",
    "            axes[1, 0].set_title(\"检索文档数量\")\n",
    "            axes[1, 0].set_xlabel(\"查询\")\n",
    "            axes[1, 0].set_ylabel(\"数量\")\n",
    "            axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # 图表4: 累积令牌\n",
    "        cumulative_tokens = np.cumsum([h[\"metrics\"].get(\"total_tokens\", 0) for h in self.history])\n",
    "        axes[1, 1].plot(queries, cumulative_tokens, marker='^', color=\"brown\", alpha=0.7)\n",
    "        axes[1, 1].set_title(\"累积令牌使用量\")\n",
    "        axes[1, 1].set_xlabel(\"查询\")\n",
    "        axes[1, 1].set_ylabel(\"总令牌数\")\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad46775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 系统实现\n",
    "# =========================\n",
    "\n",
    "class SimpleRAG(RAGSystem):\n",
    "    \"\"\"\n",
    "    使用嵌入进行相似性搜索的简单 RAG 系统。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"初始化简单 RAG 系统。\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # 文档是否已嵌入\n",
    "        self.documents_embedded = False\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        向文档存储添加文档并重置嵌入标志。\n",
    "        \n",
    "        Args:\n",
    "            documents: 要添加的 Document 对象列表\n",
    "        \"\"\"\n",
    "        super().add_documents(documents)\n",
    "        self.documents_embedded = False\n",
    "    \n",
    "    def _ensure_documents_embedded(self) -> None:\n",
    "        \"\"\"确保所有文档都有嵌入。\"\"\"\n",
    "        if self.documents_embedded:\n",
    "            return\n",
    "        \n",
    "        docs_to_embed = [doc for doc in self.documents if doc.embedding is None]\n",
    "        \n",
    "        if docs_to_embed:\n",
    "            self._log(f\"为 {len(docs_to_embed)} 个文档生成嵌入\")\n",
    "            extract_document_batch_embeddings(\n",
    "                docs_to_embed, \n",
    "                client=self.client,\n",
    "                model=self.embedding_model\n",
    "            )\n",
    "        \n",
    "        self.documents_embedded = True\n",
    "    \n",
    "    def _retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = DEFAULT_TOP_K\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        使用嵌入相似性检索查询的相关文档。\n",
    "        \n",
    "        Args:\n",
    "            query: 查询字符串\n",
    "            top_k: 返回的结果数量\n",
    "            \n",
    "        Returns:\n",
    "            list: (文档, 相似度分数) 元组列表\n",
    "        \"\"\"\n",
    "        # 确保文档已嵌入\n",
    "        self._ensure_documents_embedded()\n",
    "        \n",
    "        if not self.documents:\n",
    "            self._log(\"文档存储中没有文档\")\n",
    "            return []\n",
    "        \n",
    "        # 生成查询嵌入\n",
    "        query_embedding = generate_embedding(\n",
    "            query,\n",
    "            client=self.client,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        # 执行相似性搜索\n",
    "        results = similarity_search(\n",
    "            query_embedding,\n",
    "            self.documents,\n",
    "            top_k\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class ChunkedRAG(SimpleRAG):\n",
    "    \"\"\"\n",
    "    在建立索引之前对文档进行分块的 RAG 系统。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化分块 RAG 系统。\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: 每个块的最大标记数\n",
    "            chunk_overlap: 块之间重叠的标记数\n",
    "            **kwargs: 传递给 RAGSystem 的其他参数\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # 分块前的原始文档\n",
    "        self.original_documents = []\n",
    "        \n",
    "        # 是否使用 FAISS 进行检索（如果可用）\n",
    "        self.use_faiss = FAISS_AVAILABLE\n",
    "        self.faiss_index = None\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        将文档添加到存储中，对其进行分块，并重置嵌入标志。\n",
    "        \n",
    "        Args:\n",
    "            documents: 要添加的 Document 对象列表\n",
    "        \"\"\"\n",
    "        # 存储原始文档\n",
    "        self.original_documents.extend(documents)\n",
    "        \n",
    "        # 对每个文档进行分块\n",
    "        chunked_docs = []\n",
    "        for doc in documents:\n",
    "            chunks = text_to_chunks(\n",
    "                doc.content,\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "                model=self.model\n",
    "            )\n",
    "            \n",
    "            # 将元数据复制到块中并添加父引用\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk.metadata.update(doc.metadata)\n",
    "                chunk.metadata[\"parent_id\"] = doc.id\n",
    "                chunk.metadata[\"chunk_index\"] = i\n",
    "                chunk.metadata[\"parent_content\"] = doc.content[:100] + \"...\" if len(doc.content) > 100 else doc.content\n",
    "            \n",
    "            chunked_docs.extend(chunks)\n",
    "        \n",
    "        # 将分块文档添加到存储中\n",
    "        super().add_documents(chunked_docs)\n",
    "        \n",
    "        # 如果使用 FAISS，重置 FAISS 索引\n",
    "        if self.use_faiss:\n",
    "            self.faiss_index = None\n",
    "    \n",
    "    def _ensure_documents_embedded(self) -> None:\n",
    "        \"\"\"确保所有文档都有嵌入并在需要时构建 FAISS 索引。\"\"\"\n",
    "        super()._ensure_documents_embedded()\n",
    "        \n",
    "        # 如果使用 FAISS，构建 FAISS 索引\n",
    "        if self.use_faiss and self.faiss_index is None and self.documents:\n",
    "            self._log(\"正在构建 FAISS 索引\")\n",
    "            self.faiss_index = create_faiss_index(self.documents)\n",
    "    \n",
    "    def _retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = DEFAULT_TOP_K\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        使用嵌入相似性或 FAISS 检索相关文档块。\n",
    "        \n",
    "        Args:\n",
    "            query: 查询字符串\n",
    "            top_k: 返回的结果数量\n",
    "            \n",
    "        Returns:\n",
    "            list: (文档, 相似度分数) 元组列表\n",
    "        \"\"\"\n",
    "        # 确保文档已嵌入并在需要时构建 FAISS 索引\n",
    "        self._ensure_documents_embedded()\n",
    "        \n",
    "        if not self.documents:\n",
    "            self._log(\"文档存储中没有文档\")\n",
    "            return []\n",
    "        \n",
    "        # 生成查询嵌入\n",
    "        query_embedding = generate_embedding(\n",
    "            query,\n",
    "            client=self.client,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        # 如果可用，使用 FAISS 进行检索\n",
    "        if self.use_faiss and self.faiss_index is not None:\n",
    "            results = faiss_similarity_search(\n",
    "                query_embedding,\n",
    "                self.faiss_index,\n",
    "                self.documents,\n",
    "                top_k\n",
    "            )\n",
    "        else:\n",
    "            # 回退到基本相似性搜索\n",
    "            results = similarity_search(\n",
    "                query_embedding,\n",
    "                self.documents,\n",
    "                top_k\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class HybridRAG(ChunkedRAG):\n",
    "    \"\"\"\n",
    "    结合嵌入相似性与关键词搜索的 RAG 系统。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        keyword_weight: float = 0.3,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化混合 RAG 系统。\n",
    "        \n",
    "        Args:\n",
    "            keyword_weight: 关键词搜索的权重（0.0 到 1.0）\n",
    "            **kwargs: 传递给 ChunkedRAG 的其他参数\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.keyword_weight = max(0.0, min(1.0, keyword_weight))\n",
    "        self.embedding_weight = 1.0 - self.keyword_weight\n",
    "    \n",
    "    def _keyword_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Document],\n",
    "        top_k: int = DEFAULT_TOP_K\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        对文档执行关键词搜索。\n",
    "        \n",
    "        Args:\n",
    "            query: 查询字符串\n",
    "            documents: Document 对象列表\n",
    "            top_k: 返回的结果数量\n",
    "            \n",
    "        Returns:\n",
    "            list: (文档, 相似度分数) 元组列表\n",
    "        \"\"\"\n",
    "        # 简单关键词匹配\n",
    "        query_terms = set(query.lower().split())\n",
    "        \n",
    "        results = []\n",
    "        for doc in documents:\n",
    "            content = doc.content.lower()\n",
    "            \n",
    "            # 计算匹配词汇并计算分数\n",
    "            matches = sum(1 for term in query_terms if term in content)\n",
    "            score = matches / len(query_terms) if query_terms else 0.0\n",
    "            \n",
    "            results.append((doc, score))\n",
    "        \n",
    "        # 按分数排序（降序）并取前 top_k 个\n",
    "        sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        return sorted_results[:top_k]\n",
    "    \n",
    "    def _retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = DEFAULT_TOP_K\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        使用混合搜索检索相关文档块。\n",
    "        \n",
    "        Args:\n",
    "            query: 查询字符串\n",
    "            top_k: 返回的结果数量\n",
    "            \n",
    "        Returns:\n",
    "            list: (文档, 相似度分数) 元组列表\n",
    "        \"\"\"\n",
    "        # 确保文档已嵌入\n",
    "        self._ensure_documents_embedded()\n",
    "        \n",
    "        if not self.documents:\n",
    "            self._log(\"文档存储中没有文档\")\n",
    "            return []\n",
    "        \n",
    "        # 生成查询嵌入\n",
    "        query_embedding = generate_embedding(\n",
    "            query,\n",
    "            client=self.client,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        # 获取语义搜索结果\n",
    "        if self.use_faiss and self.faiss_index is not None:\n",
    "            semantic_results = faiss_similarity_search(\n",
    "                query_embedding\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
